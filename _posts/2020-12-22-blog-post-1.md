---
title: 'Deep Neural Networks on FPGA exploiting Partial Reconfiguration'
date: 2020-12-22
permalink: /posts/2020/12/blog-post-1/

tags:
  - AI 반도체
  - FPGA
  - Neural Networks
  - Partial Reconfiguration
---

I recently found that there's some 'AI hardware design contest'([AI 반도체 경연대회](https://view.asiae.co.kr/article/2020111611412444851)) going on. The contestants are given the neural network(NN) SW source code and what they need to do is implement it well on the FPGA. The final design will be evaluated based on its throughput, resource usage, design novelty, and whatever criterion the contestants think their designs excel on. It would have made more sense if the contestants come up with their own algorithms as HW and SW are tightly coupled, but the organizer must be afraid that the competition may end up in algorithm contest. 

I thought, if I exploit Partial Reconfiguration in this context, I could probably win in novelty because I utilize FPGA’s special character when FPGA is given to prototype ASIC. I am also ready to argue this approach is complementary with other efficient implementations of each layer! I wasn’t 100% buying the idea of exploiting PR in this context, so I ended up not participating in the design contest, but I share what I found about previous work done in this topic.

What is Partial Reconfiguration(PR)?
======
Partial Reconfiguration is a feature of a modern FPGAs that the part of the design is dynamically modified on runtime while the remaining of the design is still running. As can be seen in the figure below, the part of the design, which was initially configured with A1.bit, can be reconfigured with A2.bit, A3.bit or A4.bit while the “static” logic is unaffected. These bitstreams, A1.bit, A2.bit, A3.bit, and A4.bit are called partial bitstreams.

<p align="center"> <img src="./_posts/images/pr.png"> </p>
